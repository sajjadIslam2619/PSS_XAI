{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a2cc37",
   "metadata": {},
   "source": [
    "Title: Deep Learning for Suicide and Depression Identification with Unsupervised Label Correction\n",
    "\n",
    "Link: https://github.com/ayaanzhaque/SDCNL/blob/main/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85013f9e",
   "metadata": {},
   "source": [
    "Dataset description: The studyâ€™s primary dataset was collected from Reddit posts in r/Depression and r/SuicideWatch, containing 1,895 posts labeled according to subreddit membership. To validate the label correction method, the authors also used the Reddit C-SSRS dataset (500 posts labeled by psychologists using the Columbia Suicide Severity Rating Scale) and the IMDB movie review dataset (50,000 reviews for sentiment classification). Additionally, posts from r/CasualConversation were used alongside r/SuicideWatch to construct a comparison dataset for suicide vs healthy classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292bd9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 915 rows to d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\depression_dataset_2.csv\n",
      "Saved 980 rows to d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\suicide_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ========= CONFIG =========\n",
    "project_dir = Path.cwd()   # current working directory\n",
    "source_dir = project_dir / \"Data_Lake\" / \"Dataset_12\"\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "warehouse_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= LOAD DATA =========\n",
    "# Replace with your actual file name inside Dataset_12\n",
    "df = pd.read_csv(source_dir / \"combined-set.csv\")\n",
    "\n",
    "# ========= MAP LABELS =========\n",
    "def map_label(row):\n",
    "    return \"suicide\" if row[\"is_suicide\"] == 1 else \"depression\"\n",
    "\n",
    "def map_subsource(row):\n",
    "    return \"r/SuicideWatch\" if row[\"is_suicide\"] == 1 else \"r/Depression\"\n",
    "\n",
    "df[\"label\"] = df.apply(map_label, axis=1)\n",
    "df[\"sub-source\"] = df.apply(map_subsource, axis=1)\n",
    "df[\"source\"] = \"Dataset_12\"\n",
    "df[\"text\"] = df[\"selftext\"]\n",
    "\n",
    "# Keep only required columns\n",
    "df_final = df[[\"text\", \"label\", \"sub-source\", \"source\"]]\n",
    "\n",
    "# ========= FUNCTION: get unique filename =========\n",
    "def get_unique_path(base_dir: Path, base_name: str) -> Path:\n",
    "    \"\"\"Return a unique path by adding _2, _3, etc. if needed.\"\"\"\n",
    "    out_path = base_dir / base_name\n",
    "    if not out_path.exists():\n",
    "        return out_path\n",
    "    stem, ext = base_name.rsplit(\".\", 1)\n",
    "    i = 2\n",
    "    while True:\n",
    "        new_name = f\"{stem}_{i}.{ext}\"\n",
    "        out_path = base_dir / new_name\n",
    "        if not out_path.exists():\n",
    "            return out_path\n",
    "        i += 1\n",
    "\n",
    "# ========= SAVE SEPARATE FILES =========\n",
    "for class_name in df_final[\"label\"].unique():\n",
    "    subset = df_final[df_final[\"label\"] == class_name]\n",
    "    out_path = get_unique_path(warehouse_dir, f\"{class_name}_dataset.csv\")\n",
    "    subset.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(subset)} rows to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85c1109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files: ['suicide_dataset.csv']\n",
      "Total rows in input: 980\n",
      "Rows kept (<= 400 words): 903\n",
      "Rows removed (> 400 words): 77\n",
      "Deduplicated rows in output: 838\n",
      "Saved to: Data_Warehouse\\suicide_dataset_small_merged.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== CONFIG =====\n",
    "project_dir = Path.cwd()                  # run this from Data_Process\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "pattern = \"suicide_dataset*.csv\"          # look for suicide dataset files\n",
    "WORD_CAP = 400\n",
    "\n",
    "def get_unique_path(base_dir: Path, base_name: str) -> Path:\n",
    "    \"\"\"Return a unique path by adding _2, _3, ... if needed.\"\"\"\n",
    "    out_path = base_dir / base_name\n",
    "    if not out_path.exists():\n",
    "        return out_path\n",
    "    stem, ext = base_name.rsplit(\".\", 1)\n",
    "    i = 2\n",
    "    while True:\n",
    "        candidate = base_dir / f\"{stem}_{i}.{ext}\"\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        i += 1\n",
    "\n",
    "# ===== find input files =====\n",
    "files = sorted(warehouse_dir.glob(pattern))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No files matched {pattern} in {warehouse_dir}\")\n",
    "\n",
    "# ===== process and merge =====\n",
    "merged_parts = []\n",
    "total_rows = 0\n",
    "total_kept = 0\n",
    "total_removed = 0\n",
    "\n",
    "for fp in files:\n",
    "    df = pd.read_csv(fp)\n",
    "    if \"text\" not in df.columns:\n",
    "        print(f\"Skipping {fp.name} because 'text' column not found\")\n",
    "        continue\n",
    "\n",
    "    # count words\n",
    "    df[\"word_count\"] = df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "    kept = df[df[\"word_count\"] <= WORD_CAP].drop(columns=[\"word_count\"])\n",
    "    removed = len(df) - len(kept)\n",
    "\n",
    "    merged_parts.append(kept)\n",
    "    total_rows += len(df)\n",
    "    total_kept += len(kept)\n",
    "    total_removed += removed\n",
    "\n",
    "# combine\n",
    "if not merged_parts:\n",
    "    raise ValueError(\"No valid datasets to merge after filtering\")\n",
    "\n",
    "merged = pd.concat(merged_parts, ignore_index=True)\n",
    "merged = merged.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ===== save =====\n",
    "out_path = get_unique_path(warehouse_dir, \"suicide_dataset_small_merged.csv\")\n",
    "merged.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ===== report =====\n",
    "print(f\"Input files: {[fp.name for fp in files]}\")\n",
    "print(f\"Total rows in input: {total_rows}\")\n",
    "print(f\"Rows kept (<= {WORD_CAP} words): {total_kept}\")\n",
    "print(f\"Rows removed (> {WORD_CAP} words): {total_removed}\")\n",
    "print(f\"Deduplicated rows in output: {len(merged)}\")\n",
    "print(f\"Saved to: {out_path.relative_to(project_dir)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
