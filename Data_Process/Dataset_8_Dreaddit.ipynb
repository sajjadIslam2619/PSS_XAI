{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9490d92",
   "metadata": {},
   "source": [
    "Title: Dreaddit: A Reddit Dataset for Stress Analysis in Social Media\n",
    "\n",
    "Link: Direct download link avaialbe in the paper's 1st page footnote. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29424a8",
   "metadata": {},
   "source": [
    "Dataset description: The Dreaddit dataset consists of 187,444 Reddit posts collected from ten subreddits across five domains (abuse, anxiety, financial, PTSD, and social/relationships). A subset of 3,553 text segments was annotated through Amazon Mechanical Turk for stress classification, with labels indicating stressful or non-stressful content. Unlike short microblogs, Dreaddit provides long-form, multi-domain narratives, enabling deeper analysis of how stress is expressed in online communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c906ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 414 rows to d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\ptsd_dataset_2.csv\n",
      "Saved 1027 rows to d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\stress_dataset_2.csv\n",
      "Saved 416 rows to d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\anxiety_dataset_2.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ========= CONFIG =========\n",
    "project_dir = Path.cwd()\n",
    "source_dir = project_dir / \"Data_Lake\" / \"Dataset_8\"\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "warehouse_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= LOAD DATA =========\n",
    "train_df = pd.read_csv(source_dir / \"dreaddit-train.csv\")\n",
    "test_df = pd.read_csv(source_dir / \"dreaddit-test.csv\")\n",
    "\n",
    "# Combine train and test\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Keep only stress-labeled rows (label == 1)\n",
    "df = df[df[\"label\"] == 1].copy()\n",
    "\n",
    "# Map subreddits into 3 classes\n",
    "def map_class(subreddit):\n",
    "    if subreddit == \"anxiety\":\n",
    "        return \"anxiety\"\n",
    "    elif subreddit == \"ptsd\":\n",
    "        return \"ptsd\"\n",
    "    else:\n",
    "        return \"stress\"\n",
    "\n",
    "df[\"label\"] = df[\"subreddit\"].apply(map_class)\n",
    "\n",
    "# Add sub-source (original subreddit name) and source (Dataset_8)\n",
    "df[\"sub-source\"] = df[\"subreddit\"]\n",
    "df[\"source\"] = \"Dataset_8\"\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_final = df[[\"text\", \"label\", \"sub-source\", \"source\"]]\n",
    "\n",
    "# ========= FUNCTION: get unique filename =========\n",
    "def get_unique_path(base_dir: Path, base_name: str) -> Path:\n",
    "    \"\"\"Return a unique path by adding _2, _3, etc. if needed.\"\"\"\n",
    "    out_path = base_dir / base_name\n",
    "    if not out_path.exists():\n",
    "        return out_path\n",
    "    # If exists, increment suffix\n",
    "    stem, ext = base_name.rsplit(\".\", 1)\n",
    "    i = 2\n",
    "    while True:\n",
    "        new_name = f\"{stem}_{i}.{ext}\"\n",
    "        out_path = base_dir / new_name\n",
    "        if not out_path.exists():\n",
    "            return out_path\n",
    "        i += 1\n",
    "\n",
    "# ========= SAVE SEPARATE FILES =========\n",
    "for class_name in df_final[\"label\"].unique():\n",
    "    subset = df_final[df_final[\"label\"] == class_name]\n",
    "    \n",
    "    # Start with class_dataset.csv\n",
    "    out_path = get_unique_path(warehouse_dir, f\"{class_name}_dataset.csv\")\n",
    "    \n",
    "    subset.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(subset)} rows to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f137285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: [WindowsPath('d:/Sajjad-Workspace/PSS_XAI/Data_Process/Data_Warehouse/stress_dataset.csv'), WindowsPath('d:/Sajjad-Workspace/PSS_XAI/Data_Process/Data_Warehouse/stress_dataset_2.csv')]\n",
      "\n",
      "Distribution of stress data by sub-source:\n",
      "sub-source\n",
      "Work                                 1341\n",
      "Health, Fatigue, or Physical Pain     782\n",
      "Family Issues                         741\n",
      "School                                739\n",
      "Emotional Turmoil                     667\n",
      "Financial Problem                     635\n",
      "Social Relationships                  626\n",
      "Other                                 608\n",
      "Everyday Decision Making              337\n",
      "relationships                         307\n",
      "domesticviolence                      249\n",
      "survivorsofabuse                      143\n",
      "assistance                            126\n",
      "homeless                               81\n",
      "almosthomeless                         59\n",
      "stress                                 45\n",
      "food_pantry                            17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved distribution table to d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\stress_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ========= CONFIG =========\n",
    "project_dir = Path.cwd()\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "\n",
    "# Find all stress dataset files\n",
    "stress_files = list(warehouse_dir.glob(\"stress_dataset*.csv\"))\n",
    "print(\"Found files:\", stress_files)\n",
    "\n",
    "# Load and combine all stress datasets\n",
    "dfs = [pd.read_csv(f) for f in stress_files]\n",
    "df_stress = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Check distribution by sub-source (subreddit)\n",
    "freq = df_stress[\"sub-source\"].value_counts()\n",
    "\n",
    "print(\"\\nDistribution of stress data by sub-source:\")\n",
    "print(freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae13fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved shortened dataset to: d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\stres_dataset_small_merged.csv\n",
      "\n",
      "Removed counts from capped categories:\n",
      "Work                                941\n",
      "Health, Fatigue, or Physical Pain   382\n",
      "Family Issues                       341\n",
      "School                              339\n",
      "Emotional Turmoil                   267\n",
      "Financial Problem                   235\n",
      "Social Relationships                226\n",
      "\n",
      "New distribution by sub-source:\n",
      "sub-source\n",
      "Other                                608\n",
      "Emotional Turmoil                    400\n",
      "Social Relationships                 400\n",
      "School                               400\n",
      "Work                                 400\n",
      "Health, Fatigue, or Physical Pain    400\n",
      "Financial Problem                    400\n",
      "Family Issues                        400\n",
      "Everyday Decision Making             337\n",
      "relationships                        307\n",
      "domesticviolence                     249\n",
      "survivorsofabuse                     143\n",
      "assistance                           126\n",
      "homeless                              81\n",
      "almosthomeless                        59\n",
      "stress                                45\n",
      "food_pantry                           17\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== CONFIG =====\n",
    "project_dir = Path.cwd()\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "\n",
    "# Categories to cap around 400\n",
    "heavy_subsources = [\n",
    "    \"Work\",\n",
    "    \"Health, Fatigue, or Physical Pain\",\n",
    "    \"Family Issues\",\n",
    "    \"School\",\n",
    "    \"Emotional Turmoil\",\n",
    "    \"Financial Problem\",\n",
    "    \"Social Relationships\",\n",
    "]\n",
    "CAP = 400\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ===== helpers =====\n",
    "def get_unique_path(base_dir: Path, base_name: str) -> Path:\n",
    "    \"\"\"Return a unique path by adding _2, _3, ... if needed.\"\"\"\n",
    "    out_path = base_dir / base_name\n",
    "    if not out_path.exists():\n",
    "        return out_path\n",
    "    stem, ext = base_name.rsplit(\".\", 1)\n",
    "    i = 2\n",
    "    while True:\n",
    "        candidate = base_dir / f\"{stem}_{i}.{ext}\"\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        i += 1\n",
    "\n",
    "# ===== load =====\n",
    "stress_files = sorted(warehouse_dir.glob(\"stress_dataset*.csv\"))\n",
    "if not stress_files:\n",
    "    raise FileNotFoundError(\"No stress_dataset*.csv files found in Data_Warehouse\")\n",
    "dfs = [pd.read_csv(f) for f in stress_files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "if \"sub-source\" not in df.columns:\n",
    "    raise KeyError(\"Expected column 'sub-source' not found\")\n",
    "\n",
    "# optional de duplicate\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ===== downsample heavy categories =====\n",
    "parts = []\n",
    "removed_counts = {}\n",
    "\n",
    "for name, group in df.groupby(\"sub-source\", dropna=False):\n",
    "    if name in heavy_subsources and len(group) > CAP:\n",
    "        removed_counts[name] = len(group) - CAP\n",
    "        parts.append(group.sample(CAP, random_state=RANDOM_STATE))\n",
    "    else:\n",
    "        removed_counts[name] = 0\n",
    "        parts.append(group)\n",
    "\n",
    "df_out = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# ===== save =====\n",
    "out_path = get_unique_path(warehouse_dir, \"stres_dataset_small_merged.csv\")\n",
    "df_out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ===== report =====\n",
    "print(f\"Saved shortened dataset to: {out_path}\")\n",
    "print(\"\\nRemoved counts from capped categories:\")\n",
    "for k in heavy_subsources:\n",
    "    print(f\"{k:35s} {removed_counts.get(k, 0)}\")\n",
    "\n",
    "\n",
    "print(\"\\nNew distribution by sub-source:\")\n",
    "print(df_out[\"sub-source\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d617185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4772, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
