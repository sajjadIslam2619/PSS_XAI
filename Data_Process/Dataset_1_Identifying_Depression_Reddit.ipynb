{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072821f6",
   "metadata": {},
   "source": [
    "Paper Title: Identifying Depression on Reddit: The Effect of Training Data\n",
    "\n",
    "Paper Link: https://github.com/Inusette/Identifying-depression/tree/master/Data_Collector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731015c4",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Eight datasets were created/used:\n",
    "\n",
    "DSF: Depression Support Forums (Ramirez-Esparza et al., 2008)\n",
    "\n",
    "DND: Non-Depression Forums (Gorbunova, 2017)\n",
    "\n",
    "DS: Reddit Depression Support subreddit\n",
    "\n",
    "BC: Reddit Breast Cancer subreddit (control)\n",
    "\n",
    "FF: Reddit Family/Friendship advice subreddits (control)\n",
    "\n",
    "DO: Posts by diagnosed depressed authors in other subreddits\n",
    "\n",
    "ND: Posts by non-depressed authors in other subreddits\n",
    "\n",
    "AllD–AllND: Combination of all positive and negative sets\n",
    "\n",
    "Each dataset contained ~400 training posts; DO/ND also had separate test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16225dde",
   "metadata": {},
   "source": [
    "# How depression and non-depression emotions are separated?\n",
    "\n",
    "Depression (Positive Class)\n",
    "\n",
    "Depression Support Forums (DSF)\n",
    "Posts explicitly written in online forums by users self-identifying as depressed (Ramirez-Esparza et al., 2008).\n",
    "\n",
    "Reddit Depression Support Subreddit (DS)\n",
    "Posts from r/depression, where authors seek community support.\n",
    "\n",
    "Diagnosed Depression in Other Subreddits (DO)\n",
    "Posts by users who explicitly mention being diagnosed with depression (e.g., “I was just diagnosed with depression”) in r/depression.\n",
    "\n",
    "Then, the authors collected these same users’ posts in other unrelated subreddits (excluding ones like r/Anxiety, r/mentalhealth, r/depression_help).\n",
    "\n",
    "This produces depression-positive examples outside of overt depression discussions.\n",
    "\n",
    "Non-Depression (Negative Class)\n",
    "\n",
    "Non-Depression Forums (DND)\n",
    "Control group texts from forums unrelated to depression (Gorbunova, 2017).\n",
    "\n",
    "Reddit Breast Cancer Subreddit (BC)\n",
    "Chosen as a comparison group since users discuss illness but not depression.\n",
    "\n",
    "Reddit Family/Friendship Advice Subreddits (FF)\n",
    "Posts topically closer to depression but not about mental health (e.g., seeking advice on family or friendship).\n",
    "\n",
    "No Depression in Other Subreddits (ND)\n",
    "Posts from Reddit users who never posted in depression-related communities during the same timeframe.\n",
    "\n",
    "This group serves as the true control for DO (so DO vs. ND is the most realistic classification setting).\n",
    "\n",
    "Important Note\n",
    "\n",
    "For DO and ND, they ensured one post per author to avoid bias from prolific writers.\n",
    "\n",
    "Unlike Yates et al. (2017), they did not manually validate every diagnosis claim, so some false positives are possible (users might exaggerate or misuse “diagnosed with depression”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9c4682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3023 texts\n",
      "Saved to: d:\\Sajjad-Workspace\\XAI\\Data_Process\\Data_Warehouse\\depression_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Folders relative to the notebook location (Data_Process)\n",
    "project_dir = Path.cwd()\n",
    "source_dir = project_dir / \"Data_Lake\" / \"Dataset_1\"\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "warehouse_dir.mkdir(parents=True, exist_ok=True)  # ensure target exists\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Walk all subfolders under Dataset_1 and collect .txt files\n",
    "for txt_path in source_dir.rglob(\"*.txt\"):\n",
    "    try:\n",
    "        text = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "        if text:  # skip empty files\n",
    "            # optional: keep the immediate subfolder name as source\n",
    "            source_folder = txt_path.parent.name\n",
    "            rows.append({\"text\": text, \"label\": \"depression\", \"source\": source_folder})\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {txt_path}: {e}\")\n",
    "\n",
    "# Build DataFrame and save\n",
    "df = pd.DataFrame(rows, columns=[\"text\", \"label\", \"source\"])\n",
    "out_path = warehouse_dir / \"depression_dataset.csv\"\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Found {len(rows)} texts\")\n",
    "print(f\"Saved to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f98ff",
   "metadata": {},
   "source": [
    "## Temporary code to rename columns and add new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved at Data_Warehouse\\depression_dataset_final.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Correct relative path (since notebook is in Data_Process)\n",
    "in_path = Path(\"Data_Warehouse/depression_dataset.csv\")\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(in_path)\n",
    "\n",
    "# Rename 'source' → 'sub-source'\n",
    "df = df.rename(columns={\"source\": \"sub-source\"})\n",
    "\n",
    "# Add new column 'source' with constant value\n",
    "df[\"source\"] = \"dataset_1\"\n",
    "\n",
    "# Save updated CSV\n",
    "out_path = Path(\"Data_Warehouse/depression_dataset_final.csv\")\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Updated CSV saved at {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17b83883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    file  total_posts  posts_gt_400_words\n",
      "  depression_dataset.csv         3023                 371\n",
      "depression_dataset_2.csv          915                  88\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== CONFIG =====\n",
    "project_dir = Path.cwd()              # assumes you are in Data_Process\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "\n",
    "# ===== find depression datasets =====\n",
    "depression_files = sorted(warehouse_dir.glob(\"depression_dataset*.csv\"))\n",
    "if not depression_files:\n",
    "    raise FileNotFoundError(\"No depression_dataset*.csv files found in Data_Warehouse\")\n",
    "\n",
    "# ===== check each file =====\n",
    "summary = []\n",
    "\n",
    "for file in depression_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    if \"text\" not in df.columns:\n",
    "        print(f\"Skipping {file.name} (no 'text' column found)\")\n",
    "        continue\n",
    "\n",
    "    df[\"word_count\"] = df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "    long_posts = df[df[\"word_count\"] > 400]\n",
    "\n",
    "    summary.append({\n",
    "        \"file\": file.name,\n",
    "        \"total_posts\": len(df),\n",
    "        \"posts_gt_400_words\": len(long_posts)\n",
    "    })\n",
    "\n",
    "# ===== summary table =====\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380fe9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files: ['depression_dataset.csv', 'depression_dataset_2.csv']\n",
      "Total rows in input: 3938\n",
      "Rows kept (<= 400 words): 3479\n",
      "Rows removed (> 400 words): 459\n",
      "Deduplicated rows in output: 3476\n",
      "Saved to: Data_Warehouse\\depression_dataset_small_merged.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== CONFIG =====\n",
    "project_dir = Path.cwd()                  # run this from Data_Process\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "pattern = \"depression_dataset*.csv\"\n",
    "WORD_CAP = 400\n",
    "\n",
    "def get_unique_path(base_dir: Path, base_name: str) -> Path:\n",
    "    \"\"\"Return a unique path by adding _2, _3, ... if needed.\"\"\"\n",
    "    out_path = base_dir / base_name\n",
    "    if not out_path.exists():\n",
    "        return out_path\n",
    "    stem, ext = base_name.rsplit(\".\", 1)\n",
    "    i = 2\n",
    "    while True:\n",
    "        candidate = base_dir / f\"{stem}_{i}.{ext}\"\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        i += 1\n",
    "\n",
    "# ===== find input files =====\n",
    "files = sorted(warehouse_dir.glob(pattern))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No files matched {pattern} in {warehouse_dir}\")\n",
    "\n",
    "# ===== process and merge =====\n",
    "merged_parts = []\n",
    "total_rows = 0\n",
    "total_kept = 0\n",
    "total_removed = 0\n",
    "\n",
    "for fp in files:\n",
    "    df = pd.read_csv(fp)\n",
    "    if \"text\" not in df.columns:\n",
    "        print(f\"Skipping {fp.name} because 'text' column not found\")\n",
    "        continue\n",
    "\n",
    "    df[\"word_count\"] = df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "    kept = df[df[\"word_count\"] <= WORD_CAP].drop(columns=[\"word_count\"])\n",
    "    removed = len(df) - len(kept)\n",
    "\n",
    "    merged_parts.append(kept)\n",
    "    total_rows += len(df)\n",
    "    total_kept += len(kept)\n",
    "    total_removed += removed\n",
    "\n",
    "# combine\n",
    "if not merged_parts:\n",
    "    raise ValueError(\"No valid datasets to merge after filtering\")\n",
    "\n",
    "merged = pd.concat(merged_parts, ignore_index=True)\n",
    "merged = merged.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ===== save =====\n",
    "out_path = get_unique_path(warehouse_dir, \"depression_dataset_small_merged.csv\")\n",
    "merged.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ===== report =====\n",
    "print(f\"Input files: {[fp.name for fp in files]}\")\n",
    "print(f\"Total rows in input: {total_rows}\")\n",
    "print(f\"Rows kept (<= {WORD_CAP} words): {total_kept}\")\n",
    "print(f\"Rows removed (> {WORD_CAP} words): {total_removed}\")\n",
    "print(f\"Deduplicated rows in output: {len(merged)}\")\n",
    "print(f\"Saved to: {out_path.relative_to(project_dir)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
