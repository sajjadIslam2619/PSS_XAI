{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f0d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1bed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# PRAW setup\n",
    "# ==============================\n",
    "# Initialize PRAW with your credentials\n",
    "reddit = praw.Reddit(client_id='jQqFEBqOyeJcGFBvU5N3Zw', #Change Client ID based on app\n",
    "                     client_secret='miUWh8AmwnAx20CCLgjyxBrZ0rHsyQ', # Change Client Secret\n",
    "                     user_agent='Opiates_Recovery_data_scraper by /u/Sajjad_Islam',\n",
    "                     username='Sajjad_Islam',  # Your Reddit username\n",
    "                     password='patuakhali'   # Your Reddit password\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8a8ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: Sajjad_Islam\n"
     ]
    }
   ],
   "source": [
    "# Verify the connection\n",
    "try:\n",
    "    me = reddit.user.me()\n",
    "    print(f\"Authenticated as: {me}\")\n",
    "except Exception as e:\n",
    "    print(f\"Auth error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1714236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: z:\\Documents\\Projects\\PSS_XAI\\Data_Process\\Data_Lake\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Resolve base paths\n",
    "# ==============================\n",
    "try:\n",
    "    # Works when running as a .py script\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Fallback for Jupyter/interactive sessions\n",
    "    script_dir = Path.cwd()\n",
    "\n",
    "# Data_Process is one level up from reddit_data_proces\n",
    "project_dir = script_dir.parent\n",
    "OUTPUT_DIR = project_dir /\"Data_Process\"/ \"Data_Lake\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dba930ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Config\n",
    "# ==============================\n",
    "SUBREDDITS = [\"ExplainLikeImFive\", \"TodayILearned\"]\n",
    "TARGET_PER_SUB = 2000\n",
    "BATCH_SIZE = 100\n",
    "PAUSE_SECONDS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f57dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Helper to fetch latest posts\n",
    "# ==============================\n",
    "def fetch_latest_posts(subreddit_name, target=2000, batch_size=100, pause=2):\n",
    "    sr = reddit.subreddit(subreddit_name)\n",
    "    collected = []\n",
    "    seen_ids = set()\n",
    "    after = None\n",
    "    total_fetched = 0\n",
    "\n",
    "    while len(collected) < target:\n",
    "        submissions = list(sr.new(limit=batch_size, params={\"after\": after}))\n",
    "        if not submissions:\n",
    "            print(f\"No more posts available for r/{subreddit_name}.\")\n",
    "            break\n",
    "\n",
    "        for sub in submissions:\n",
    "            if sub.id in seen_ids:\n",
    "                continue\n",
    "            seen_ids.add(sub.id)\n",
    "            collected.append({\n",
    "                \"id\": sub.id,\n",
    "                \"title\": sub.title,\n",
    "                \"author\": sub.author.name if sub.author else \"[deleted]\",\n",
    "                \"selftext\": sub.selftext or \"\",\n",
    "                \"score\": sub.score,\n",
    "                \"ups\": sub.ups,\n",
    "                \"num_comments\": sub.num_comments,\n",
    "                \"created_utc\": sub.created_utc,\n",
    "                \"permalink\": f\"https://www.reddit.com{sub.permalink}\",\n",
    "                \"url\": sub.url,\n",
    "                \"subreddit\": subreddit_name\n",
    "            })\n",
    "            if len(collected) >= target:\n",
    "                break\n",
    "\n",
    "        after = submissions[-1].fullname\n",
    "        total_fetched += len(submissions)\n",
    "        print(f\"r/{subreddit_name}: collected {len(collected)} so far (fetched {total_fetched} raw).\")\n",
    "        time.sleep(pause)\n",
    "\n",
    "    df = pd.DataFrame(collected)\n",
    "    if not df.empty and \"created_utc\" in df.columns:\n",
    "        df = df.sort_values(\"created_utc\", ascending=False).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09986865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching latest posts from r/ExplainLikeImFive...\n",
      "r/ExplainLikeImFive: collected 100 so far (fetched 100 raw).\n",
      "r/ExplainLikeImFive: collected 200 so far (fetched 200 raw).\n",
      "r/ExplainLikeImFive: collected 300 so far (fetched 300 raw).\n",
      "r/ExplainLikeImFive: collected 400 so far (fetched 400 raw).\n",
      "r/ExplainLikeImFive: collected 500 so far (fetched 500 raw).\n",
      "r/ExplainLikeImFive: collected 600 so far (fetched 600 raw).\n",
      "r/ExplainLikeImFive: collected 700 so far (fetched 700 raw).\n",
      "r/ExplainLikeImFive: collected 800 so far (fetched 800 raw).\n",
      "r/ExplainLikeImFive: collected 900 so far (fetched 900 raw).\n",
      "r/ExplainLikeImFive: collected 924 so far (fetched 924 raw).\n",
      "No more posts available for r/ExplainLikeImFive.\n",
      "Saved 924 rows to z:\\Documents\\Projects\\PSS_XAI\\Data_Process\\Data_Lake\\ExplainLikeImFive_latest_posts.csv\n",
      "\n",
      "Fetching latest posts from r/TodayILearned...\n",
      "r/TodayILearned: collected 100 so far (fetched 100 raw).\n",
      "r/TodayILearned: collected 200 so far (fetched 200 raw).\n",
      "r/TodayILearned: collected 300 so far (fetched 300 raw).\n",
      "r/TodayILearned: collected 400 so far (fetched 400 raw).\n",
      "r/TodayILearned: collected 500 so far (fetched 500 raw).\n",
      "r/TodayILearned: collected 600 so far (fetched 600 raw).\n",
      "r/TodayILearned: collected 700 so far (fetched 700 raw).\n",
      "r/TodayILearned: collected 800 so far (fetched 800 raw).\n",
      "r/TodayILearned: collected 900 so far (fetched 900 raw).\n",
      "r/TodayILearned: collected 934 so far (fetched 934 raw).\n",
      "No more posts available for r/TodayILearned.\n",
      "Saved 934 rows to z:\\Documents\\Projects\\PSS_XAI\\Data_Process\\Data_Lake\\TodayILearned_latest_posts.csv\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Run and save\n",
    "# ==============================\n",
    "for sr_name in SUBREDDITS:\n",
    "    print(f\"\\nFetching latest posts from r/{sr_name}...\")\n",
    "    df = fetch_latest_posts(sr_name, target=TARGET_PER_SUB, batch_size=BATCH_SIZE, pause=PAUSE_SECONDS)\n",
    "    out_path = OUTPUT_DIR / f\"{sr_name}_latest_posts.csv\"\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d029418",
   "metadata": {},
   "source": [
    "### Clean data for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b62b5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved paths:\n",
      "  CWD:         z:\\Documents\\Projects\\PSS_XAI\\reddit_data_proces\n",
      "  Data_Process: \\\\venn.mscsnet.mu.edu\\accounts\\kislam\\Documents\\Projects\\PSS_XAI\\Data_Process\n",
      "  Input dir:   \\\\venn.mscsnet.mu.edu\\accounts\\kislam\\Documents\\Projects\\PSS_XAI\\Data_Process\\Data_Lake\n",
      "  Output dir:  \\\\venn.mscsnet.mu.edu\\accounts\\kislam\\Documents\\Projects\\PSS_XAI\\Data_Process\\Data_Warehouse\n",
      "Reading \\\\venn.mscsnet.mu.edu\\accounts\\kislam\\Documents\\Projects\\PSS_XAI\\Data_Process\\Data_Lake\\ExplainLikeImFive_latest_posts.csv\n",
      "Reading \\\\venn.mscsnet.mu.edu\\accounts\\kislam\\Documents\\Projects\\PSS_XAI\\Data_Process\\Data_Lake\\TodayILearned_latest_posts.csv\n",
      "Saved 1858 rows to \\\\venn.mscsnet.mu.edu\\accounts\\kislam\\Documents\\Projects\\PSS_XAI\\Data_Process\\Data_Warehouse\\no_mental_condition_dataset_own.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ============== locate Data_Process robustly ==============\n",
    "def find_data_process(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    # case 1: you are already inside Data_Process\n",
    "    if start.name == \"Data_Process\":\n",
    "        return start\n",
    "    # walk up and return the first folder that is named Data_Process\n",
    "    for p in [start, *start.parents]:\n",
    "        if p.name == \"Data_Process\":\n",
    "            return p\n",
    "        # also handle running from PSS_XAI or deeper\n",
    "        candidate = p / \"Data_Process\"\n",
    "        if candidate.exists() and candidate.is_dir():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Could not find a folder named Data_Process above the current path.\")\n",
    "\n",
    "# works in script and notebook\n",
    "try:\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "\n",
    "DATA_PROCESS = find_data_process(script_dir)\n",
    "\n",
    "input_dir  = DATA_PROCESS / \"Data_Lake\"\n",
    "output_dir = DATA_PROCESS / \"Data_Warehouse\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Resolved paths:\")\n",
    "print(\"  CWD:        \", Path.cwd())\n",
    "print(\"  Data_Process:\", DATA_PROCESS)\n",
    "print(\"  Input dir:  \", input_dir)\n",
    "print(\"  Output dir: \", output_dir)\n",
    "\n",
    "# ============== load inputs ==============\n",
    "files_expected = [\n",
    "    input_dir / \"ExplainLikeImFive_latest_posts.csv\",\n",
    "    input_dir / \"TodayILearned_latest_posts.csv\",\n",
    "]\n",
    "\n",
    "# if any expected file is missing, try to auto discover\n",
    "existing = [f for f in files_expected if f.exists()]\n",
    "if len(existing) < 2:\n",
    "    discovered = sorted(input_dir.glob(\"*_latest_posts.csv\"))\n",
    "    print(f\"Discovered in Data_Lake: {[p.name for p in discovered]}\")\n",
    "    if discovered:\n",
    "        existing = discovered\n",
    "\n",
    "if not existing:\n",
    "    raise FileNotFoundError(\"No input CSVs found in Data_Lake. Make sure the crawl step saved files there.\")\n",
    "\n",
    "dfs = []\n",
    "for f in existing:\n",
    "    print(f\"Reading {f}\")\n",
    "    df_src = pd.read_csv(f)\n",
    "    if df_src.empty:\n",
    "        print(f\"Warning: {f.name} is empty. Skipping.\")\n",
    "        continue\n",
    "    df_out = pd.DataFrame({\n",
    "        \"text\": df_src.get(\"selftext\", \"\").fillna(\"\"),\n",
    "        \"label\": \"none\",\n",
    "        \"sub-source\": df_src.get(\"subreddit\", \"\").fillna(\"\"),\n",
    "        \"source\": \"Dataset_own_1\",\n",
    "    })\n",
    "    dfs.append(df_out)\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"All input CSVs were empty after loading. Nothing to merge.\")\n",
    "\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "out_path = output_dir / \"no_mental_condition_dataset_own.csv\"\n",
    "merged.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved {len(merged)} rows to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d4b3f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\no_mental_condition_dataset_own.csv\n",
      "Total rows in input: 1858\n",
      "Rows kept (<= 400 words): 708\n",
      "Rows removed (> 400 words): 1\n",
      "Deduplicated rows in output: 708\n",
      "Saved to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\no_mental_condition_dataset_own_small.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== CONFIG =====\n",
    "input_file = Path(r\"D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\no_mental_condition_dataset_own.csv\")\n",
    "WORD_CAP = 400\n",
    "\n",
    "def get_unique_path(base_path: Path) -> Path:\n",
    "    \"\"\"Return a unique path by adding _2, _3, ... if needed.\"\"\"\n",
    "    if not base_path.exists():\n",
    "        return base_path\n",
    "    stem, ext = base_path.stem, base_path.suffix\n",
    "    i = 2\n",
    "    while True:\n",
    "        candidate = base_path.with_name(f\"{stem}_{i}{ext}\")\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        i += 1\n",
    "\n",
    "# ===== load input =====\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"{input_file} not found\")\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "if \"text\" not in df.columns:\n",
    "    raise ValueError(\"'text' column not found in input file\")\n",
    "\n",
    "# ===== filter =====\n",
    "df[\"word_count\"] = df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "kept = df[df[\"word_count\"] <= WORD_CAP].drop(columns=[\"word_count\"])\n",
    "removed = len(df) - len(kept)\n",
    "\n",
    "# ===== deduplicate =====\n",
    "kept = kept.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ===== save =====\n",
    "out_file = get_unique_path(input_file.with_name(\"no_mental_condition_dataset_own_small.csv\"))\n",
    "kept.to_csv(out_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ===== report =====\n",
    "print(f\"Input file: {input_file}\")\n",
    "print(f\"Total rows in input: {len(df)}\")\n",
    "print(f\"Rows kept (<= {WORD_CAP} words): {len(kept)}\")\n",
    "print(f\"Rows removed (> {WORD_CAP} words): {removed}\")\n",
    "print(f\"Deduplicated rows in output: {len(kept)}\")\n",
    "print(f\"Saved to: {out_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
